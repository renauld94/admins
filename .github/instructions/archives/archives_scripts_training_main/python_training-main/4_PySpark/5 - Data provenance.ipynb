{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e04391e-378f-41c8-9f0b-0414c7275869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Guidance on data provenance/lineage\n",
    "\n",
    "A key aspect in D4U is the data lineage between listings (core, basic etc). Each record is uniquely identified with a set a columns (aka data provenance) and we know its record/table origin, if any.\n",
    "\n",
    "The system not only expects these columns but also consistency overtime in the data lineage. If not, saving a listing to the Silver layer will fail.\n",
    "\n",
    "We'll discuss below two concrete scenarios where this could happen: using non-deterministic code and aggregating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ecb6a2e-3db2-4130-94fa-7d3a18ea1dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1) Non-deterministic code\n",
    "\n",
    "A non-deterministic code is a program that does not produce the same outcome in each run. \n",
    "\n",
    "The issue arises with operations like dropDuplicates without a consistent order, leading to different outputs in each run. To ensure deterministic results, you need to sort the DataFrame by all relevant columns, including any unique identifiers. By adding a sorting step before deduplication, you can guarantee consistent outcomes across multiple executions.\n",
    "\n",
    "If your code is non-deterministic, saving the dataframe to the Silver layer will probably fail.\n",
    "\n",
    "Consider this df:\n",
    "\n",
    "| SUBJID | PARAM | AGE | REC_ID        |\n",
    "|--------|-------|-----|---------------|\n",
    "| 1      | test1 | 28  | 784574124541  |\n",
    "| 1      | test2 | 28  | 163746845476  |\n",
    "| 2      | test1 | 35  | 8132687656497 |\n",
    "| 2      | test2 | 35  | 687423121313  |\n",
    "\n",
    "Say you want to use dropDuplicates() and keep only SUBJID, AGE and REC_ID. \n",
    "\n",
    "To save to Silver, you need to carry the data provenance columns (i.e., REC_ID in this fictional example) and make sure the REC_ID values are consistent overtime. Without sorting before dropDuplicates() you cannot guarantee that the same record will be consistently kept, which will prevent from saving to Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6f5dc9-43e3-4f69-bdb9-8899770c6890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window, Row\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a6e881-0469-4c7a-8279-8acee02367cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To showcase the issue, in the next cells I'll create a dataframe, inflate it, shuffle it and perform non-deterministic operations on several iterations. \n",
    "# The following is a function that calculate the MD5 checksum (somekind of a fingerprint) of a PySpark dataframe. I'll use that for comparing outcomes.\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def calculate_md5(df):\n",
    "    ''' \n",
    "    Takes a PySpark dataframe and returns its MD5 sum.\n",
    "    First we convert to Pandas df, then to a string, finally we calculate the MD5 sum.\n",
    "    '''\n",
    "\n",
    "    df_string = df.toPandas().to_string(index=False)\n",
    "\n",
    "    return hashlib.md5(df_string.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d64ebff-c70a-4bfb-8765-31760138a4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create some 'large' test data, as we are more likely to see the problem on a large dataset  \n",
    "\n",
    "# Example data\n",
    "data    = [(\"John\", 34), (\"Bob\", 40), (\"John\", 53), (\"Bob\", 45)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Inflate the dataset x1000 by adding an ID column (values: 0 to 999)\n",
    "inflated_df = df.crossJoin(spark.range(0, 1000))\n",
    "\n",
    "# Shuffle the dataframe\n",
    "inflated_df = inflated_df.orderBy(F.rand())\n",
    "\n",
    "inflated_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99beb3ca-a94d-45e7-a4a4-f430cfe5c1d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropDuplicates() is a non-deterministic operation, as it depends on the order of the rows. Let's demonstrate this.\n",
    "\n",
    "def perform_operations(\n",
    "    df,\n",
    "    sort_beforehand: bool\n",
    "):\n",
    "    \n",
    "    if sort_beforehand:\n",
    "        df = df.orderBy(\"Name\", 'ID')\n",
    "\n",
    "    md5_checksums = []\n",
    "\n",
    "    # Let's iterate 10 times, to see if we get the same MD5 sum each time\n",
    "    for _ in range(10):\n",
    "\n",
    "        # Perform non-deterministic operations\n",
    "        df_dedup = df.dropDuplicates(subset=[\"Name\"])\n",
    "        \n",
    "        # Calculate MD5 checksum\n",
    "        md5_checksum = calculate_md5(df_dedup)\n",
    "        md5_checksums.append(md5_checksum)\n",
    "\n",
    "    # Display the MD5 checksums\n",
    "    for i, checksum in enumerate(md5_checksums):\n",
    "        print(f\"Iteration {i+1}: MD5 Checksum = {checksum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82f9040-a3c2-4b73-b6cf-bca879edd516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's run the operations without sorting\n",
    "perform_operations(inflated_df, sort_beforehand=False)\n",
    "\n",
    "# See below? Different MD5 sums. Not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a57302d-abd3-4d77-9b72-a1da35eedd07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's run the operations with sorting\n",
    "perform_operations(inflated_df, sort_beforehand=True)\n",
    "\n",
    "# See below? Always the same sum. Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8887e85e-ccda-4776-b55e-f8dc4da305ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Take home message**: remember to sort your data before <code>dropDuplicates()</code>, <code>first</code>, <code>last</code> or any other operation that requires uniqueness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccac8ce5-c6bc-4f47-9f8f-6deccd95e31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2) Data provenance when aggregating data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df20e36-4391-454b-b178-42aefcbc58c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When we aggregate data, we can lose the data provenance. Without it, standard Marvel listings will fail, specifically addDataProvenance().\n",
    "# Say we want to add to DM domain the average HR by subject, coming from VS. We want to push this later on to the silver layer.\n",
    "\n",
    "basic_DM = spark.createDataFrame([\n",
    "    Row(SUBJID='1', AGE=25, SEX='M', DM_URECID='1657912664161'),\n",
    "    Row(SUBJID='2', AGE=44, SEX='F', DM_URECID='9873131643217'),\n",
    "])\n",
    "\n",
    "basic_VS = spark.createDataFrame([\n",
    "    Row(SUBJID='1', VISIT=1, HR=80,  VS_URECID='103241324165'),\n",
    "    Row(SUBJID='1', VISIT=2, HR=85,  VS_URECID='237566541315'),\n",
    "    Row(SUBJID='2', VISIT=1, HR=120, VS_URECID='787465415615'),\n",
    "    Row(SUBJID='2', VISIT=2, HR=75,  VS_URECID='023413354787'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff62c5c-4ebb-4a4b-b241-ea809a68cf0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating the data using groupBy/agg will not keep the data provenance. We do NOT recommend using it in that scenario\n",
    "VS_summary = basic_VS.groupBy(\"SUBJID\").agg(F.mean(\"HR\").alias(\"Mean_HR\"))\n",
    "\n",
    "# See, at this point, data provenance (VS_URECID in my example) was lost:\n",
    "VS_summary.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af45468-1ce6-4d7f-a34b-264504129087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating the data using partitionBy is the best way here, since we don't lose the data provenance. \n",
    "\n",
    "window = Window.partitionBy('SUBJID')\n",
    "basic_VS = basic_VS.withColumn(\n",
    "    'HR_mean',\n",
    "    F.mean(\"HR\").over(window)\n",
    ")\n",
    "\n",
    "basic_VS.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c39e4d-db60-47ab-aea3-6024eb7effc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If we need to keep only one record, we can always use dropDuplicates(), but remember to sort before to avoid non-deterministic issues:\n",
    "basic_VS.orderBy('VS_URECID').dropDuplicates(['SUBJID']).display()\n",
    "\n",
    "# We can merge this onto DM by SUBJID, and we'll have provenance from both DM and VS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffba4baf-20b4-4b49-8ece-67dfa0baa724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In some other scenarios (e.g. pivot/unpivot), it won't be possible to use partitionBy. In that case, we need to bring back arbritrary provenance. \n",
    "# Let's filter VS to keep only one record for each SUBJID.\n",
    "\n",
    "# This is one way. Create a rank column, and filter on it.\n",
    "VS_unique  = basic_VS.withColumn('rank', F.rank().over(Window.partitionBy('SUBJID').orderBy(['SUBJID', 'VS_URECID']))) # Deterministic sort, otherwise the result will not be deterministic and saving to silver layer will fail!\n",
    "VS_unique = VS_unique.filter(F.col('rank') == 1).drop('rank')\n",
    "\n",
    "# We have provenance info\n",
    "VS_unique.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0473b725-1dfa-4834-8dae-c43191b960cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge the summary against this nodup VS:\n",
    "df = VS_unique.join(VS_summary, \"SUBJID\", \"left\")\n",
    "\n",
    "# We have now a summary with data provenance, though arbritrary. And we can merge this onto DM, and get data provenance from both domains.\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5 - Data provenance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
