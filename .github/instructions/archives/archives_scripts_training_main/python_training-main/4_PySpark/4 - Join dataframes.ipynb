{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "739b04a6-5875-4299-b109-304a13fbf554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Join PySpark dataframes\n",
    "\n",
    "- [Merging guide](https://www.cojolt.io/blog/joining-merging-data-with-pyspark-a-complete-guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e97e15b-0bf5-4cf1-8563-db1d7c9d7be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's import PySpark and initialize the Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02099217-7956-492b-89e4-867dcdfcefe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's create 2 df, so we can merge them later\n",
    "from datetime import date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Horizontal/wide df\n",
    "df1 = spark.createDataFrame([\n",
    "    Row(SUBJID='1', AGE=25, SEX='F',  HEIGHT= 150, SCREENED=date(2024, 1, 15)),\n",
    "    Row(SUBJID='2', AGE=56, SEX='M',  HEIGHT=168, SCREENED=date(2024, 2, 10)),\n",
    "    Row(SUBJID='3', AGE=44, SEX=None, HEIGHT=170, SCREENED=date(2024, 1, 17))\n",
    "])\n",
    "\n",
    "# Vertical/narrow df\n",
    "df2 =  spark.createDataFrame([\n",
    "    Row(SUBJID='1', PARAM='Param1', VALUE='12'   ),\n",
    "    Row(SUBJID='1', PARAM='Param2', VALUE='Hello'),\n",
    "    Row(SUBJID='1', PARAM='Param3', VALUE=True   ),\n",
    "    Row(SUBJID='3', PARAM='Param1', VALUE='14'   ),\n",
    "    Row(SUBJID='3', PARAM='Param2', VALUE='Bye'),\n",
    "    Row(SUBJID='3', PARAM='Param3', VALUE=False   ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989c602f-ba41-43cd-a8ae-06339227cb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's do a left join, using SUBJID as key for merging\n",
    "merged_df = df2.join(df1, df1.SUBJID == df2.SUBJID, \"left\")\n",
    "\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f6539e-a26c-4b5d-ba41-bfa08e157555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merging PySpark DataFrames can be achieved through multiple techniques. Below are different methods and their scenarios.\n",
    "\n",
    "# 1. Inner Join (Default)\n",
    "    # Selects records that have matching values in both DataFrames.\n",
    "joined_df = df1.join(df2, df1.SUBJID == df2.SUBJID, \"inner\")\n",
    "\n",
    "# 2. Left Join\n",
    "    # Selects all records from the left DataFrame, and the matched records from the right DataFrame. \n",
    "    # The result is NULL from the right side if there is no match.\n",
    "left_joined_df = df1.join(df2, df1.SUBJID == df2.SUBJID, \"left\")\n",
    "\n",
    "# 3. Right Join\n",
    "    # Selects all records from the right DataFrame, and the matched records from the left DataFrame. \n",
    "    # The result is NULL from the left side when there is no match.\n",
    "right_joined_df = df1.join(df2, df1.SUBJID == df2.SUBJID, \"right\")\n",
    "\n",
    "# 4. Full Outer Join\n",
    "    # Returns records when there is a match in one of the DataFrames.\n",
    "outer_joined_df = df1.join(df2, df1.SUBJID == df2.SUBJID, \"outer\")\n",
    "\n",
    "# 5. Cross Join\n",
    "    # Returns the Cartesian product of both DataFrames.\n",
    "cross_joined_df = df1.crossJoin(df2)\n",
    "\n",
    "# 6. Using SQL for joining\n",
    "df1.createOrReplaceTempView(\"table1\")\n",
    "df2.createOrReplaceTempView(\"table2\")\n",
    "\n",
    "sql_query = 'SELECT * FROM table1 t1 JOIN table2 t2 ON t1.SUBJID = t2.SUBJID'\n",
    "sql_joined_df = spark.sql(sql_query)\n",
    "\n",
    "# 7. Broadcast Join\n",
    "    # Broadcasting in PySpark is a way to optimize joins when one of the DataFrames is small enough to fit in memory.\n",
    "    # By broadcasting the smaller DataFrame, we can avoid shuffling large amounts of data across the cluster.\n",
    "from pyspark.sql.functions import broadcast\n",
    "broadcast_joined_df = df1.join(broadcast(df2), df1.SUBJID == df2.SUBJID, \"inner\")\n",
    "\n",
    "# 8. Natural Join\n",
    "    # Automatic match on same column names, use with caution\n",
    "natural_joined_df = df1.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ecb6e3-7bf6-4603-beba-4c51b2c3608e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = left_joined_df\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284f9e56-0691-45ab-94fb-bf2613b39071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# After the merge, the BY columns are not coalesced into one column! This is different from Pandas behavior. See the column names, we have 2 SUBJID columns!\n",
    "df.columns\n",
    "\n",
    "# PySpark maintains metadata internally to manage column names and references. However, there is no direct API to explicitly retrieve the origin of each column in the merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e7214d-9db4-422b-abcf-1858b9f5a546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If we wanted to use 'SUBJID', we would have to quality if (eg. df1['SUBJID']. Instead we could get rid of the extra SUBJID columns, the one coming from the df2 dataframe:\n",
    "df = df.drop(df2['SUBJID'])\n",
    "print(df.columns)\n",
    "\n",
    "# Only one SUBJID remains. We don't need anymore to qualify it when using it "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4 - Join dataframes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
