{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4cb0ce",
   "metadata": {},
   "source": [
    "# Session 1.15: String Processing and Text Analysis\n",
    "\n",
    "## **Essential for Healthcare Text Data and PySpark String Functions**\n",
    "\n",
    "### **Learning Objectives**\n",
    "By the end of this session, you will:\n",
    "- Master advanced string manipulation techniques\n",
    "- Process healthcare text data effectively\n",
    "- Apply string methods essential for PySpark text processing\n",
    "- Build text analysis pipelines for clinical data\n",
    "\n",
    "---\n",
    "\n",
    "### **Relevance to PySpark**\n",
    "String processing is fundamental in PySpark for cleaning text data, parsing structured text, and preparing data for analysis. These skills directly translate to PySpark's string functions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b3df1",
   "metadata": {},
   "source": [
    "## 1. Advanced String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcare text data examples\n",
    "patient_notes = [\n",
    "    \"Patient: John Doe, Age: 45, Chief Complaint: Chest pain and shortness of breath\",\n",
    "    \"Patient: Jane Smith, Age: 32, Chief Complaint: Headache, nausea, and dizziness\",\n",
    "    \"Patient: Bob Johnson, Age: 58, Chief Complaint: Joint pain in knees and back\",\n",
    "    \"Patient: Alice Brown, Age: 28, Chief Complaint: Persistent cough and fever\"\n",
    "]\n",
    "\n",
    "medication_data = [\n",
    "    \"Lisinopril 10mg - Take once daily for blood pressure\",\n",
    "    \"Metformin 500mg - Take twice daily with meals for diabetes\",\n",
    "    \"Ibuprofen 200mg - Take as needed for pain, max 3 times daily\",\n",
    "    \"Omeprazole 20mg - Take once daily before breakfast for acid reflux\"\n",
    "]\n",
    "\n",
    "# String cleaning and normalization\n",
    "def clean_patient_name(name_string):\n",
    "    \"\"\"Clean and normalize patient names.\"\"\"\n",
    "    # Remove extra whitespace and convert to title case\n",
    "    cleaned = ' '.join(name_string.strip().split())\n",
    "    \n",
    "    # Handle common abbreviations\n",
    "    cleaned = cleaned.replace(' Jr.', ' Jr').replace(' Sr.', ' Sr')\n",
    "    cleaned = cleaned.replace(' III', ' III').replace(' II', ' II')\n",
    "    \n",
    "    return cleaned.title()\n",
    "\n",
    "def extract_patient_info(note):\n",
    "    \"\"\"Extract structured information from patient notes.\"\"\"\n",
    "    # Extract patient name\n",
    "    name_start = note.find(\"Patient: \") + len(\"Patient: \")\n",
    "    name_end = note.find(\", Age:\")\n",
    "    patient_name = note[name_start:name_end]\n",
    "    \n",
    "    # Extract age\n",
    "    age_start = note.find(\"Age: \") + len(\"Age: \")\n",
    "    age_end = note.find(\", Chief Complaint:\")\n",
    "    age = int(note[age_start:age_end])\n",
    "    \n",
    "    # Extract chief complaint\n",
    "    complaint_start = note.find(\"Chief Complaint: \") + len(\"Chief Complaint: \")\n",
    "    chief_complaint = note[complaint_start:]\n",
    "    \n",
    "    return {\n",
    "        'name': clean_patient_name(patient_name),\n",
    "        'age': age,\n",
    "        'chief_complaint': chief_complaint.strip()\n",
    "    }\n",
    "\n",
    "print(\"Extracting Patient Information:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for note in patient_notes:\n",
    "    patient_info = extract_patient_info(note)\n",
    "    print(f\"Name: {patient_info['name']}\")\n",
    "    print(f\"Age: {patient_info['age']}\")\n",
    "    print(f\"Complaint: {patient_info['chief_complaint']}\")\n",
    "    print()\n",
    "\n",
    "# String validation and formatting\n",
    "def validate_medication_format(med_string):\n",
    "    \"\"\"Validate medication string format.\"\"\"\n",
    "    # Check for required components: medication name, dosage, instructions\n",
    "    if ' - ' not in med_string:\n",
    "        return False, \"Missing instructions separator\"\n",
    "    \n",
    "    medication_part, instructions = med_string.split(' - ', 1)\n",
    "    \n",
    "    # Check for dosage pattern (number + mg/g/ml)\n",
    "    import re\n",
    "    dosage_pattern = r'\\d+(?:\\.\\d+)?(?:mg|g|ml|mcg)'\n",
    "    if not re.search(dosage_pattern, medication_part):\n",
    "        return False, \"Missing or invalid dosage format\"\n",
    "    \n",
    "    if len(instructions.strip()) < 5:\n",
    "        return False, \"Instructions too short\"\n",
    "    \n",
    "    return True, \"Valid format\"\n",
    "\n",
    "print(\"Medication Format Validation:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for med in medication_data:\n",
    "    is_valid, message = validate_medication_format(med)\n",
    "    status = \"✓\" if is_valid else \"✗\"\n",
    "    print(f\"{status} {med}\")\n",
    "    if not is_valid:\n",
    "        print(f\"   Error: {message}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7873dc1",
   "metadata": {},
   "source": [
    "## 2. Regular Expressions for Healthcare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135038e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample clinical text data\n",
    "clinical_notes = [\n",
    "    \"Patient presented on 2024-01-15 with BP 140/90 mmHg, HR 85 bpm, temp 98.6°F\",\n",
    "    \"Lab results: Glucose 145 mg/dL, Cholesterol 220 mg/dL, HbA1c 7.2%\",\n",
    "    \"Prescribed: Lisinopril 10mg QD, Metformin 500mg BID with meals\",\n",
    "    \"Follow-up scheduled for 2024-02-15 at 2:30 PM in cardiology clinic\",\n",
    "    \"Patient weight: 185 lbs, height: 5'10\\\", BMI calculated at 26.5\"\n",
    "]\n",
    "\n",
    "# Regular expression patterns for healthcare data\n",
    "patterns = {\n",
    "    'date': r'\\d{4}-\\d{2}-\\d{2}',\n",
    "    'blood_pressure': r'\\d{2,3}/\\d{2,3}\\s*mmHg',\n",
    "    'heart_rate': r'\\d{2,3}\\s*bpm',\n",
    "    'temperature': r'\\d{2,3}\\.\\d\\s*°F',\n",
    "    'lab_value': r'\\w+\\s+\\d+(?:\\.\\d+)?\\s*(?:mg/dL|%)',\n",
    "    'medication': r'[A-Za-z]+\\s+\\d+mg\\s+(?:QD|BID|TID|QID)',\n",
    "    'time': r'\\d{1,2}:\\d{2}\\s*(?:AM|PM)',\n",
    "    'weight': r'\\d+(?:\\.\\d+)?\\s*lbs',\n",
    "    'height': r\"\\d'\\d+\\\"\",\n",
    "    'bmi': r'BMI.*?\\d+\\.\\d+'\n",
    "}\n",
    "\n",
    "def extract_clinical_data(text, pattern_name):\n",
    "    \"\"\"Extract clinical data using regular expressions.\"\"\"\n",
    "    pattern = patterns.get(pattern_name)\n",
    "    if not pattern:\n",
    "        return []\n",
    "    \n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    return matches\n",
    "\n",
    "def parse_vital_signs(text):\n",
    "    \"\"\"Parse vital signs from clinical text.\"\"\"\n",
    "    vitals = {}\n",
    "    \n",
    "    # Blood pressure\n",
    "    bp_match = re.search(r'BP\\s+(\\d{2,3})/(\\d{2,3})', text, re.IGNORECASE)\n",
    "    if bp_match:\n",
    "        vitals['systolic'] = int(bp_match.group(1))\n",
    "        vitals['diastolic'] = int(bp_match.group(2))\n",
    "    \n",
    "    # Heart rate\n",
    "    hr_match = re.search(r'HR\\s+(\\d{2,3})', text, re.IGNORECASE)\n",
    "    if hr_match:\n",
    "        vitals['heart_rate'] = int(hr_match.group(1))\n",
    "    \n",
    "    # Temperature\n",
    "    temp_match = re.search(r'temp\\s+(\\d{2,3}\\.\\d)', text, re.IGNORECASE)\n",
    "    if temp_match:\n",
    "        vitals['temperature'] = float(temp_match.group(1))\n",
    "    \n",
    "    return vitals\n",
    "\n",
    "def parse_lab_results(text):\n",
    "    \"\"\"Parse laboratory results from clinical text.\"\"\"\n",
    "    lab_results = {}\n",
    "    \n",
    "    # Find all lab values\n",
    "    lab_pattern = r'(\\w+)\\s+(\\d+(?:\\.\\d+)?)\\s*(mg/dL|%)'\n",
    "    matches = re.findall(lab_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    for test_name, value, unit in matches:\n",
    "        lab_results[test_name.lower()] = {\n",
    "            'value': float(value),\n",
    "            'unit': unit\n",
    "        }\n",
    "    \n",
    "    return lab_results\n",
    "\n",
    "def parse_medications(text):\n",
    "    \"\"\"Parse medication information from clinical text.\"\"\"\n",
    "    medications = []\n",
    "    \n",
    "    # Pattern for medication with dosage and frequency\n",
    "    med_pattern = r'([A-Za-z]+)\\s+(\\d+)mg\\s+(QD|BID|TID|QID)'\n",
    "    matches = re.findall(med_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    frequency_map = {\n",
    "        'QD': 'Once daily',\n",
    "        'BID': 'Twice daily', \n",
    "        'TID': 'Three times daily',\n",
    "        'QID': 'Four times daily'\n",
    "    }\n",
    "    \n",
    "    for med_name, dosage, frequency in matches:\n",
    "        medications.append({\n",
    "            'name': med_name.title(),\n",
    "            'dosage': f\"{dosage}mg\",\n",
    "            'frequency': frequency_map.get(frequency.upper(), frequency)\n",
    "        })\n",
    "    \n",
    "    return medications\n",
    "\n",
    "print(\"Clinical Data Extraction with Regular Expressions:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for i, note in enumerate(clinical_notes, 1):\n",
    "    print(f\"\\nNote {i}: {note}\")\n",
    "    print(\"Extracted data:\")\n",
    "    \n",
    "    # Extract different types of data\n",
    "    dates = extract_clinical_data(note, 'date')\n",
    "    if dates:\n",
    "        print(f\"  Dates: {dates}\")\n",
    "    \n",
    "    vitals = parse_vital_signs(note)\n",
    "    if vitals:\n",
    "        print(f\"  Vital signs: {vitals}\")\n",
    "    \n",
    "    labs = parse_lab_results(note)\n",
    "    if labs:\n",
    "        print(f\"  Lab results: {labs}\")\n",
    "    \n",
    "    meds = parse_medications(note)\n",
    "    if meds:\n",
    "        print(f\"  Medications: {meds}\")\n",
    "    \n",
    "    times = extract_clinical_data(note, 'time')\n",
    "    if times:\n",
    "        print(f\"  Times: {times}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef51042",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample messy healthcare text data\n",
    "messy_clinical_data = [\n",
    "    \"   Patient: JOHN    DOE,  age: 45   Chief complaint:    chest pain    \",\n",
    "    \"Diagnosis: Type II Diabetes Mellitus, Hypertension, hyperlipidemia\",\n",
    "    \"MEDICATIONS: lisinopril   10mg,  metformin 500MG,  simvastatin  20mg\",\n",
    "    \"Allergies: PCN (penicillin),  NKDA, shellfish   \",\n",
    "    \"Social Hx: Smokes 1 ppd x 20 years, ETOH: 2-3 drinks/week, denies illicit drugs\"\n",
    "]\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize and clean clinical text data.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Standardize case for certain patterns\n",
    "    text = re.sub(r'\\bmg\\b', 'mg', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bml\\b', 'mL', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Standardize medical abbreviations\n",
    "    abbreviations = {\n",
    "        r'\\bPCN\\b': 'Penicillin',\n",
    "        r'\\bNKDA\\b': 'No Known Drug Allergies',\n",
    "        r'\\bETOH\\b': 'Alcohol',\n",
    "        r'\\bppd\\b': 'packs per day',\n",
    "        r'\\bHx\\b': 'History',\n",
    "        r'\\bType II\\b': 'Type 2'\n",
    "    }\n",
    "    \n",
    "    for abbrev, expansion in abbreviations.items():\n",
    "        text = re.sub(abbrev, expansion, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def standardize_medication_names(text):\n",
    "    \"\"\"Standardize medication names to proper case.\"\"\"\n",
    "    # Common medication names with proper capitalization\n",
    "    med_corrections = {\n",
    "        'lisinopril': 'Lisinopril',\n",
    "        'metformin': 'Metformin',\n",
    "        'simvastatin': 'Simvastatin',\n",
    "        'amlodipine': 'Amlodipine',\n",
    "        'omeprazole': 'Omeprazole',\n",
    "        'ibuprofen': 'Ibuprofen'\n",
    "    }\n",
    "    \n",
    "    for incorrect, correct in med_corrections.items():\n",
    "        pattern = r'\\b' + re.escape(incorrect) + r'\\b'\n",
    "        text = re.sub(pattern, correct, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_structured_data(text):\n",
    "    \"\"\"Extract structured data from normalized clinical text.\"\"\"\n",
    "    structured_data = {}\n",
    "    \n",
    "    # Extract patient name\n",
    "    name_match = re.search(r'Patient:\\s*([^,]+)', text, re.IGNORECASE)\n",
    "    if name_match:\n",
    "        structured_data['patient_name'] = name_match.group(1).strip().title()\n",
    "    \n",
    "    # Extract age\n",
    "    age_match = re.search(r'age:\\s*(\\d+)', text, re.IGNORECASE)\n",
    "    if age_match:\n",
    "        structured_data['age'] = int(age_match.group(1))\n",
    "    \n",
    "    # Extract diagnoses\n",
    "    diagnosis_match = re.search(r'Diagnosis:\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "    if diagnosis_match:\n",
    "        diagnoses = [d.strip() for d in diagnosis_match.group(1).split(',')]\n",
    "        structured_data['diagnoses'] = diagnoses\n",
    "    \n",
    "    # Extract medications\n",
    "    med_match = re.search(r'MEDICATIONS:\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "    if med_match:\n",
    "        medications = [m.strip() for m in med_match.group(1).split(',')]\n",
    "        structured_data['medications'] = medications\n",
    "    \n",
    "    # Extract allergies\n",
    "    allergy_match = re.search(r'Allergies:\\s*([^\\n]+)', text, re.IGNORECASE)\n",
    "    if allergy_match:\n",
    "        allergies = [a.strip() for a in allergy_match.group(1).split(',')]\n",
    "        structured_data['allergies'] = allergies\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "print(\"Text Cleaning and Normalization:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, messy_text in enumerate(messy_clinical_data, 1):\n",
    "    print(f\"\\nOriginal {i}: {messy_text}\")\n",
    "    \n",
    "    # Clean and normalize\n",
    "    normalized = normalize_text(messy_text)\n",
    "    normalized = standardize_medication_names(normalized)\n",
    "    \n",
    "    print(f\"Cleaned  {i}: {normalized}\")\n",
    "    \n",
    "    # Extract structured data if possible\n",
    "    structured = extract_structured_data(normalized)\n",
    "    if structured:\n",
    "        print(f\"Structured: {structured}\")\n",
    "\n",
    "# Demonstrate text preprocessing pipeline\n",
    "def clinical_text_preprocessing_pipeline(text_list):\n",
    "    \"\"\"Complete text preprocessing pipeline for clinical data.\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        # Step 1: Basic cleaning\n",
    "        cleaned = normalize_text(text)\n",
    "        \n",
    "        # Step 2: Standardize medical terms\n",
    "        standardized = standardize_medication_names(cleaned)\n",
    "        \n",
    "        # Step 3: Extract structured data\n",
    "        structured = extract_structured_data(standardized)\n",
    "        \n",
    "        # Step 4: Add metadata\n",
    "        processed_record = {\n",
    "            'original_text': text,\n",
    "            'cleaned_text': standardized,\n",
    "            'structured_data': structured,\n",
    "            'processing_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        processed_data.append(processed_record)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "print(\"\\n\\nComplete Preprocessing Pipeline:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "pipeline_results = clinical_text_preprocessing_pipeline(messy_clinical_data[:2])\n",
    "\n",
    "for i, result in enumerate(pipeline_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Original: {result['original_text'][:50]}...\")\n",
    "    print(f\"  Cleaned: {result['cleaned_text']}\")\n",
    "    print(f\"  Structured: {result['structured_data']}\")\n",
    "    print(f\"  Processed: {result['processing_timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08831ce5",
   "metadata": {},
   "source": [
    "## 4. Text Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4fbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Sample clinical text corpus\n",
    "clinical_corpus = [\n",
    "    \"Patient presents with acute myocardial infarction. Administered aspirin and nitroglycerin.\",\n",
    "    \"Chronic obstructive pulmonary disease exacerbation. Prescribed bronchodilators and steroids.\",\n",
    "    \"Type 2 diabetes mellitus with poor glycemic control. Increased Metformin dosage.\",\n",
    "    \"Hypertension well controlled on current medications. Continue Lisinopril therapy.\",\n",
    "    \"Patient reports chest pain and shortness of breath. Ordered cardiac enzymes and ECG.\",\n",
    "    \"Pneumonia confirmed by chest X-ray. Started on antibiotic therapy immediately.\",\n",
    "    \"Acute kidney injury secondary to dehydration. Initiated fluid resuscitation protocol.\",\n",
    "    \"Diabetic ketoacidosis treated with insulin infusion and fluid replacement therapy.\"\n",
    "]\n",
    "\n",
    "def tokenize_medical_text(text):\n",
    "    \"\"\"Tokenize medical text, preserving medical terms.\"\"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation but preserve hyphens in medical terms\n",
    "    translator = str.maketrans('', '', string.punctuation.replace('-', ''))\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # Split into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def calculate_text_statistics(text_list):\n",
    "    \"\"\"Calculate comprehensive text statistics.\"\"\"\n",
    "    all_tokens = []\n",
    "    sentence_lengths = []\n",
    "    word_lengths = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        tokens = tokenize_medical_text(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        sentence_lengths.append(len(tokens))\n",
    "        word_lengths.extend([len(token) for token in tokens])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_tokens = len(all_tokens)\n",
    "    unique_tokens = len(set(all_tokens))\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    avg_word_length = sum(word_lengths) / len(word_lengths)\n",
    "    \n",
    "    # Most common words\n",
    "    word_counts = Counter(all_tokens)\n",
    "    most_common = word_counts.most_common(10)\n",
    "    \n",
    "    return {\n",
    "        'total_documents': len(text_list),\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': unique_tokens,\n",
    "        'lexical_diversity': unique_tokens / total_tokens,\n",
    "        'avg_sentence_length': round(avg_sentence_length, 2),\n",
    "        'avg_word_length': round(avg_word_length, 2),\n",
    "        'most_common_words': most_common\n",
    "    }\n",
    "\n",
    "def extract_medical_entities(text_list):\n",
    "    \"\"\"Extract common medical entities from text.\"\"\"\n",
    "    # Define medical term patterns\n",
    "    conditions_pattern = r'\\b(?:hypertension|diabetes|pneumonia|asthma|copd|myocardial infarction|kidney injury|ketoacidosis)\\b'\n",
    "    medications_pattern = r'\\b(?:aspirin|metformin|lisinopril|insulin|nitroglycerin|bronchodilators|steroids|antibiotics?)\\b'\n",
    "    procedures_pattern = r'\\b(?:ecg|x-ray|ct scan|mri|blood test|cardiac enzymes)\\b'\n",
    "    \n",
    "    all_text = ' '.join(text_list).lower()\n",
    "    \n",
    "    conditions = re.findall(conditions_pattern, all_text, re.IGNORECASE)\n",
    "    medications = re.findall(medications_pattern, all_text, re.IGNORECASE)\n",
    "    procedures = re.findall(procedures_pattern, all_text, re.IGNORECASE)\n",
    "    \n",
    "    return {\n",
    "        'conditions': Counter(conditions),\n",
    "        'medications': Counter(medications),\n",
    "        'procedures': Counter(procedures)\n",
    "    }\n",
    "\n",
    "def analyze_treatment_patterns(text_list):\n",
    "    \"\"\"Analyze treatment patterns in clinical text.\"\"\"\n",
    "    treatment_keywords = {\n",
    "        'medication': ['prescribed', 'administered', 'started', 'given', 'continued'],\n",
    "        'procedure': ['ordered', 'performed', 'conducted', 'scheduled'],\n",
    "        'therapy': ['therapy', 'treatment', 'protocol', 'intervention']\n",
    "    }\n",
    "    \n",
    "    pattern_counts = {category: 0 for category in treatment_keywords}\n",
    "    \n",
    "    for text in text_list:\n",
    "        text_lower = text.lower()\n",
    "        for category, keywords in treatment_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                pattern_counts[category] += text_lower.count(keyword)\n",
    "    \n",
    "    return pattern_counts\n",
    "\n",
    "print(\"Clinical Text Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate text statistics\n",
    "stats = calculate_text_statistics(clinical_corpus)\n",
    "print(\"Text Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if key != 'most_common_words':\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nMost Common Words:\")\n",
    "for word, count in stats['most_common_words']:\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Extract medical entities\n",
    "print(\"\\nMedical Entity Extraction:\")\n",
    "entities = extract_medical_entities(clinical_corpus)\n",
    "\n",
    "for entity_type, entity_counts in entities.items():\n",
    "    print(f\"\\n{entity_type.title()}:\")\n",
    "    for entity, count in entity_counts.most_common():\n",
    "        print(f\"  {entity}: {count}\")\n",
    "\n",
    "# Analyze treatment patterns\n",
    "print(\"\\nTreatment Pattern Analysis:\")\n",
    "treatment_patterns = analyze_treatment_patterns(clinical_corpus)\n",
    "for pattern, count in treatment_patterns.items():\n",
    "    print(f\"  {pattern.title()} mentions: {count}\")\n",
    "\n",
    "# Document similarity analysis\n",
    "def calculate_document_similarity(doc1, doc2):\n",
    "    \"\"\"Calculate similarity between two documents using Jaccard similarity.\"\"\"\n",
    "    tokens1 = set(tokenize_medical_text(doc1))\n",
    "    tokens2 = set(tokenize_medical_text(doc2))\n",
    "    \n",
    "    intersection = tokens1.intersection(tokens2)\n",
    "    union = tokens1.union(tokens2)\n",
    "    \n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "print(\"\\nDocument Similarity Analysis:\")\n",
    "print(\"(Using first 3 documents for demo)\")\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(i+1, 3):\n",
    "        similarity = calculate_document_similarity(clinical_corpus[i], clinical_corpus[j])\n",
    "        print(f\"Document {i+1} vs Document {j+1}: {similarity:.3f} similarity\")\n",
    "        print(f\"  Doc {i+1}: {clinical_corpus[i][:50]}...\")\n",
    "        print(f\"  Doc {j+1}: {clinical_corpus[j][:50]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbfb42e",
   "metadata": {},
   "source": [
    "## 5. String Performance and Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "def string_performance_comparison():\n",
    "    \"\"\"Compare different string operation performance patterns.\"\"\"\n",
    "    \n",
    "    # Sample data\n",
    "    patient_ids = [f\"PT{i:06d}\" for i in range(1000)]\n",
    "    \n",
    "    print(\"String Performance Comparison:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Method 1: String concatenation with +\n",
    "    start_time = time.time()\n",
    "    result1 = \"\"\n",
    "    for pid in patient_ids:\n",
    "        result1 += pid + \",\"\n",
    "    result1 = result1.rstrip(\",\")\n",
    "    concat_time = time.time() - start_time\n",
    "    \n",
    "    # Method 2: Using join()\n",
    "    start_time = time.time()\n",
    "    result2 = \",\".join(patient_ids)\n",
    "    join_time = time.time() - start_time\n",
    "    \n",
    "    # Method 3: Using list comprehension and join\n",
    "    start_time = time.time()\n",
    "    result3 = \",\".join([pid for pid in patient_ids])\n",
    "    list_comp_time = time.time() - start_time\n",
    "    \n",
    "    # Method 4: Using generator and join\n",
    "    start_time = time.time()\n",
    "    result4 = \",\".join(pid for pid in patient_ids)\n",
    "    generator_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"String concatenation (+): {concat_time:.6f} seconds\")\n",
    "    print(f\"Join method:             {join_time:.6f} seconds\")\n",
    "    print(f\"List comprehension:      {list_comp_time:.6f} seconds\")\n",
    "    print(f\"Generator expression:    {generator_time:.6f} seconds\")\n",
    "    \n",
    "    print(f\"\\nPerformance improvement with join(): {concat_time/join_time:.1f}x faster\")\n",
    "    \n",
    "    # Verify results are identical\n",
    "    assert result1 == result2 == result3 == result4\n",
    "    print(\"✓ All methods produce identical results\")\n",
    "\n",
    "def memory_efficient_text_processing():\n",
    "    \"\"\"Demonstrate memory-efficient text processing techniques.\"\"\"\n",
    "    \n",
    "    def process_large_text_file_inefficient(lines):\n",
    "        \"\"\"Inefficient approach - loads everything into memory.\"\"\"\n",
    "        all_data = []\n",
    "        for line in lines:\n",
    "            # Simulate processing\n",
    "            processed = line.strip().upper()\n",
    "            all_data.append(processed)\n",
    "        return all_data\n",
    "    \n",
    "    def process_large_text_file_efficient(lines):\n",
    "        \"\"\"Efficient approach - processes line by line.\"\"\"\n",
    "        for line in lines:\n",
    "            # Simulate processing\n",
    "            processed = line.strip().upper()\n",
    "            yield processed\n",
    "    \n",
    "    # Generate sample data\n",
    "    sample_lines = [f\"Patient record {i}: medical data here\\n\" for i in range(1000)]\n",
    "    \n",
    "    print(\"\\nMemory Usage Comparison:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Inefficient method\n",
    "    start_time = time.time()\n",
    "    inefficient_result = process_large_text_file_inefficient(sample_lines)\n",
    "    inefficient_time = time.time() - start_time\n",
    "    inefficient_memory = sys.getsizeof(inefficient_result)\n",
    "    \n",
    "    # Efficient method\n",
    "    start_time = time.time()\n",
    "    efficient_result = process_large_text_file_efficient(sample_lines)\n",
    "    # Process first 5 items to demonstrate\n",
    "    first_five = [next(efficient_result) for _ in range(5)]\n",
    "    efficient_time = time.time() - start_time\n",
    "    efficient_memory = sys.getsizeof(efficient_result)\n",
    "    \n",
    "    print(f\"Inefficient approach:\")\n",
    "    print(f\"  Time: {inefficient_time:.6f} seconds\")\n",
    "    print(f\"  Memory: {inefficient_memory:,} bytes\")\n",
    "    print(f\"  Records: {len(inefficient_result)}\")\n",
    "    \n",
    "    print(f\"\\nEfficient approach:\")\n",
    "    print(f\"  Time (first 5): {efficient_time:.6f} seconds\")\n",
    "    print(f\"  Memory: {efficient_memory:,} bytes\")\n",
    "    print(f\"  Sample: {first_five[0] if first_five else 'None'}\")\n",
    "    \n",
    "    memory_savings = ((inefficient_memory - efficient_memory) / inefficient_memory) * 100\n",
    "    print(f\"\\nMemory savings: {memory_savings:.1f}%\")\n",
    "\n",
    "# Run performance comparisons\n",
    "string_performance_comparison()\n",
    "memory_efficient_text_processing()\n",
    "\n",
    "# String interning demonstration\n",
    "def string_interning_demo():\n",
    "    \"\"\"Demonstrate string interning for memory optimization.\"\"\"\n",
    "    \n",
    "    print(\"\\nString Interning for Medical Codes:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Common medical codes that appear frequently\n",
    "    common_codes = ['ICD10-E11.9', 'ICD10-I10', 'ICD10-J44.1', 'CPT-99213']\n",
    "    \n",
    "    # Without interning - new string objects\n",
    "    codes_without_interning = []\n",
    "    for _ in range(100):\n",
    "        for code in common_codes:\n",
    "            codes_without_interning.append(str(code))  # Force new string creation\n",
    "    \n",
    "    # With interning - reuse string objects\n",
    "    codes_with_interning = []\n",
    "    for _ in range(100):\n",
    "        for code in common_codes:\n",
    "            codes_with_interning.append(sys.intern(code))\n",
    "    \n",
    "    # Check memory usage\n",
    "    memory_without = sys.getsizeof(codes_without_interning)\n",
    "    memory_with = sys.getsizeof(codes_with_interning)\n",
    "    \n",
    "    print(f\"Without interning: {memory_without:,} bytes\")\n",
    "    print(f\"With interning:    {memory_with:,} bytes\")\n",
    "    \n",
    "    # Test object identity\n",
    "    code1 = sys.intern('ICD10-E11.9')\n",
    "    code2 = sys.intern('ICD10-E11.9')\n",
    "    print(f\"\\nInterned strings are identical objects: {code1 is code2}\")\n",
    "    print(f\"This enables fast comparison and reduced memory usage\")\n",
    "\n",
    "string_interning_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd0ca7",
   "metadata": {},
   "source": [
    "## 6. Practice Exercise\n",
    "\n",
    "Build a comprehensive clinical text processing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3cd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Clinical Notes Processing System\n",
    "# Build a system that processes clinical notes and extracts structured information\n",
    "\n",
    "sample_clinical_notes = [\n",
    "    \"Patient: John Smith, DOB: 1975-03-15, MRN: 123456. Chief complaint: severe chest pain radiating to left arm, onset 2 hours ago. VS: BP 160/95, HR 105, RR 22, Temp 98.8F. Assessment: Rule out acute MI. Plan: ECG, cardiac enzymes, aspirin 325mg, nitro SL PRN.\",\n",
    "    \"Patient: Mary Johnson, DOB: 1968-07-22, MRN: 789012. Chief complaint: SOB and ankle swelling x 3 days. PMH: CHF, HTN, DM. Meds: Lisinopril 10mg daily, Metformin 500mg BID. VS: BP 145/88, HR 92, RR 24, O2 sat 88% RA. Plan: CXR, BNP, increase Lisinopril to 20mg daily.\",\n",
    "    \"Patient: Robert Davis, DOB: 1982-11-08, MRN: 345678. Chief complaint: persistent cough and fever x 5 days. VS: BP 125/78, HR 88, RR 20, Temp 101.2F. Physical exam: crackles bilateral lower lobes. Assessment: Community acquired pneumonia. Plan: Chest X-ray, CBC, Azithromycin 500mg daily x 5 days.\"\n",
    "]\n",
    "\n",
    "# TODO: Create functions for:\n",
    "# 1. extract_patient_demographics(note) - Extract name, DOB, MRN\n",
    "# 2. extract_vital_signs(note) - Extract all vital signs\n",
    "# 3. extract_medications(note) - Extract medication names, doses, frequencies\n",
    "# 4. extract_chief_complaint(note) - Extract and normalize chief complaint\n",
    "# 5. extract_assessment_and_plan(note) - Extract clinical assessment and treatment plan\n",
    "# 6. generate_structured_summary(note) - Combine all extractions into structured format\n",
    "# 7. batch_process_notes(notes_list) - Process multiple notes efficiently\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3ea1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this session, you learned:\n",
    "- ✅ Advanced string manipulation for healthcare data\n",
    "- ✅ Regular expressions for clinical text parsing\n",
    "- ✅ Text cleaning and normalization techniques\n",
    "- ✅ Clinical text analysis and statistics\n",
    "- ✅ String performance optimization strategies\n",
    "- ✅ Memory-efficient text processing patterns\n",
    "- ✅ Essential skills for PySpark string functions\n",
    "\n",
    "**Next:** Session 1.16 - APIs and JSON Data Processing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
