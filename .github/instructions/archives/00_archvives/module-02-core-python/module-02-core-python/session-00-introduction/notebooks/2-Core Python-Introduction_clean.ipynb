{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6232f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "362c005f",
   "metadata": {},
   "source": [
    "# Python Academy\n",
    "\n",
    "## **By Johnson & Johnson | D4U â€“  Platform Development**\n",
    "\n",
    "### **Data Skills Transformation Program**\n",
    "\n",
    "As part of J&J's D4U initiative, we are building scalable, future-proof data workflows. This academy provides comprehensive training in Python and PySpark - Databricks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Outcomes**\n",
    "\n",
    "Upon completion of this academy, participants will:\n",
    "- Master Python fundamentals for healthcare data processing\n",
    "- Utilize PySpark for distributed computing environments\n",
    "- Operate effectively within the Databricks platform\n",
    "- Apply industry best practices for clinical data management\n",
    "- Develop automated data pipelines for regulatory compliance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a21dd2",
   "metadata": {},
   "source": [
    "### **Curriculum Overview**\n",
    "\n",
    "| **Module** | **Focus Area** | **Duration** | **Key Skills** |\n",
    "|------------|----------------|--------------|-----------------|\n",
    "| **Module 1** | Core Python Foundations | 2h 00m | Variables, Functions, Classes, Data Structures, Exception Handling, APIs, File Operations, Context Managers |\n",
    "\n",
    "\n",
    "**Total Learning Time: 5 hours**\n",
    "\n",
    "---\n",
    "\n",
    "### **Module 1: Core Python Foundations - Complete Session List (2h 00m)**\n",
    "\n",
    "| **Session** | **Title** | **Status** | **Relevance to PySpark** |\n",
    "|-------------|-----------|------------|---------------------------|\n",
    "| **1.1** | Basics | Existing | Essential foundation (variables, expressions, types) |\n",
    "| **1.2** | Modules and Packages | Existing | Supports modularity, used in PySpark imports |\n",
    "| **1.3** | Data Structures | Existing | Core for manipulating lists, dicts, comprehensions |\n",
    "| **1.4** | Advanced Data Structures | Expand | Important for handling nested schemas in DataFrames |\n",
    "| **1.5** | Conditions and Loops | Existing | Critical for control flow and PySpark logic |\n",
    "| **1.6** | Functions | Existing | Includes lambda, map, filter â€“ core concepts in PySpark transformations |\n",
    "| **1.7** | Dates and Times | Existing | Important for timestamp data handling |\n",
    "| **1.8** | Regular Expressions | Existing | Useful for data cleaning and parsing text |\n",
    "| **1.9** | Classes | Existing | Builds understanding of structure and abstraction |\n",
    "| **1.10** | Decorators | Existing | Supports advanced functions, relevant for modular pipelines |\n",
    "| **1.11** | Virtual Environments | Existing | Good practice for environment isolation |\n",
    "| **1.12** | Exception Handling | Add | Needed for building robust PySpark data pipelines |\n",
    "| **1.13** | Context Managers and File I/O | Add | Essential for reading/writing files and managing resources |\n",
    "| **1.14** | Iterators and Generators | Expand | Maps directly to lazy evaluation in PySpark |\n",
    "| **1.15** | String Processing | Add | Important for cleaning and manipulating string data |\n",
    "| **1.16** | APIs and JSON | Add | Common for ingesting data from APIs and parsing input files |\n",
    "\n",
    "---\n",
    "\n",
    "### **Healthcare Data Applications**\n",
    "\n",
    "All exercises and examples focus on real-world scenarios:\n",
    "- Patient data processing and statistical analysis\n",
    "- Clinical trial data management and validation\n",
    "- Drug efficacy studies and regulatory reporting\n",
    "- Hospital operations optimization\n",
    "- Healthcare analytics and compliance reporting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0649e",
   "metadata": {},
   "source": [
    "### **Program Features**\n",
    "\n",
    "This academy provides comprehensive learning materials designed for healthcare professionals:\n",
    "\n",
    "#### **Practical Training Components**\n",
    "- Real healthcare scenarios from clinical trial environments\n",
    "- Interactive coding exercises with immediate validation\n",
    "- Data visualization techniques for regulatory submissions\n",
    "- Industry-standard workflows from J&J data engineering teams\n",
    "\n",
    "#### **Assessment and Validation**\n",
    "- Topic-specific knowledge assessments\n",
    "- Hands-on exercises applying concepts to clinical problems\n",
    "- Progress tracking throughout the learning journey\n",
    "- Competency validation for each skill area\n",
    "\n",
    "#### **Professional Support**\n",
    "- Comprehensive reference documentation\n",
    "- Code examples for practical implementation\n",
    "- Review exercises to reinforce critical concepts\n",
    "- Access to subject matter experts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a8f3a",
   "metadata": {},
   "source": [
    "### **Environment Setup & Validation**\n",
    "\n",
    "Ensure your Databricks environment is properly configured for the academy. Execute the following cells to validate your setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e82c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Python Environment Validation\n",
    "import sys\n",
    "import platform\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Python Environment Validation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Validation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nCore Python environment is ready for use.\")\n",
    "\n",
    "# 2. Test Basic Data Structures\n",
    "sample_healthcare_data = {\n",
    "    \"patient_id\": [1001, 1002, 1003],\n",
    "    \"treatment_group\": [\"Drug_A\", \"Drug_B\", \"Placebo\"],\n",
    "    \"efficacy_score\": [8.5, 7.2, 3.1]\n",
    "}\n",
    "\n",
    "print(\"\\nSample Healthcare Data Structure:\")\n",
    "for field, values in sample_healthcare_data.items():\n",
    "    print(f\"  {field}: {values}\")\n",
    "\n",
    "print(\"\\nData structures are functioning correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PySpark Environment Validation\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"D4U_Academy_Environment_Check\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"PySpark Environment Validation\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    \n",
    "    # For Spark Connect compatibility, use different approach to get app name\n",
    "    try:\n",
    "        app_name = spark.sparkContext.appName\n",
    "        print(f\"Application Name: {app_name}\")\n",
    "    except Exception:\n",
    "        # Spark Connect doesn't support sparkContext, use alternative\n",
    "        print(\"Application Name: D4U_Academy_Environment_Check (Spark Connect Mode)\")\n",
    "    \n",
    "    # Create sample healthcare DataFrame for validation\n",
    "    healthcare_schema = StructType([\n",
    "        StructField(\"patient_id\", IntegerType(), True),\n",
    "        StructField(\"hospital_site\", StringType(), True),\n",
    "        StructField(\"primary_diagnosis\", StringType(), True),\n",
    "        StructField(\"length_of_stay\", IntegerType(), True),\n",
    "        StructField(\"treatment_cost\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    sample_clinical_data = [\n",
    "        (1001, \"General Hospital\", \"Type 2 Diabetes\", 3, 15750.50),\n",
    "        (1002, \"City Medical Center\", \"Hypertension\", 2, 8920.00),\n",
    "        (1003, \"Regional Medical Center\", \"Cardiovascular Disease\", 7, 42300.75)\n",
    "    ]\n",
    "    \n",
    "    clinical_df = spark.createDataFrame(sample_clinical_data, healthcare_schema)\n",
    "    \n",
    "    print(\"\\nSample Clinical Data DataFrame:\")\n",
    "    clinical_df.show()\n",
    "    \n",
    "    print(\"PySpark environment is ready for distributed processing.\")\n",
    "    print(\"DataFrame operations are functioning correctly.\")\n",
    "    print(\"âœ… Spark Connect mode detected - optimized for cloud environments\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"PySpark validation failed: {str(e)}\")\n",
    "    print(\"Please ensure PySpark is properly configured in your environment.\")\n",
    "    print(\"Note: If using Spark Connect, some legacy APIs may not be available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093acb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Databricks Platform Validation\n",
    "try:\n",
    "    import os\n",
    "    \n",
    "    print(\"Databricks Platform Validation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check Databricks runtime environment\n",
    "    if 'DATABRICKS_RUNTIME_VERSION' in os.environ:\n",
    "        print(f\"Databricks Runtime Version: {os.environ['DATABRICKS_RUNTIME_VERSION']}\")\n",
    "        print(\"Running in Databricks managed environment.\")\n",
    "    else:\n",
    "        print(\"Not running in Databricks environment (local development mode)\")\n",
    "    \n",
    "    # Detect Spark Connect mode\n",
    "    try:\n",
    "        # Try to access sparkContext - will fail in Spark Connect\n",
    "        spark.sparkContext\n",
    "        spark_mode = \"Standard Spark Session\"\n",
    "    except Exception:\n",
    "        spark_mode = \"Spark Connect Mode (Cloud-optimized)\"\n",
    "    \n",
    "    print(f\"Spark Connection Mode: {spark_mode}\")\n",
    "    \n",
    "    # Test Databricks display function\n",
    "    try:\n",
    "        platform_status = [\n",
    "            {\"Component\": \"Python Core\", \"Status\": \"Ready\"},\n",
    "            {\"Component\": \"PySpark Engine\", \"Status\": \"Ready\"},\n",
    "            {\"Component\": \"Databricks Utils\", \"Status\": \"Ready\"},\n",
    "            {\"Component\": \"Healthcare Data Examples\", \"Status\": \"Ready\"},\n",
    "            {\"Component\": \"Spark Mode\", \"Status\": spark_mode}\n",
    "        ]\n",
    "        \n",
    "        status_df = spark.createDataFrame(platform_status)\n",
    "        \n",
    "        # Try to use display function (Databricks-specific)\n",
    "        try:\n",
    "            display(status_df)\n",
    "            print(\"âœ… Databricks display functionality is operational.\")\n",
    "        except NameError:\n",
    "            # Fallback to standard show() if display() not available\n",
    "            status_df.show(truncate=False)\n",
    "            print(\"âœ… Standard DataFrame display working (display() function not available)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DataFrame creation failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nEnvironment validation completed successfully.\")\n",
    "    print(\"Platform is ready for D4U Python Academy training.\")\n",
    "    print(\"ðŸš€ Spark Connect provides enhanced security and performance for cloud workloads\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Databricks validation encountered an issue: {str(e)}\")\n",
    "    print(\"This may be normal depending on your execution environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdf24f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Structured Learning Path**\n",
    "\n",
    "Complete modules sequentially for optimal skill development:\n",
    "\n",
    "#### **Module 1: Core Python Foundations** (2h 00m)\n",
    "- **Session 1.1-1.3:** Python Basics and Data Structures\n",
    "- **Session 1.4-1.8:** Advanced Structures and Data Handling\n",
    "- **Session 1.9-1.12:** Classes, Decorators, and Exception Handling\n",
    "- **Session 1.13-1.16:** File I/O, Context Managers, APIs, and JSON\n",
    "- **Assessment:** Comprehensive Python Knowledge Validation\n",
    "\n",
    "#### **Module 2: PySpark Fundamentals** (2h 15m)\n",
    "- **Exercise 2.1:** Introduction to Distributed Computing\n",
    "- **Exercise 2.2:** PySpark DataFrames and Core Operations\n",
    "- **Exercise 2.3:** Transformations and Actions\n",
    "- **Assessment:** PySpark Fundamentals Proficiency Check\n",
    "\n",
    "#### **Module 3: PySpark Advanced** (1h 30m)\n",
    "- **Exercise 3.1:** Complex Operations and Window Functions\n",
    "- **Exercise 3.2:** Joins and Aggregations\n",
    "- **Exercise 3.3:** Performance Tuning and Optimization\n",
    "- **Assessment:** Final Capstone Project\n",
    "\n",
    "---\n",
    "\n",
    "### **Program Initiation**\n",
    "\n",
    "**Begin with Module 1: Core Python Foundations**\n",
    "\n",
    "This streamlined training program supports J&J's **D4U initiative** for next-generation healthcare data solutions and regulatory compliance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Professional Support Resources**\n",
    "\n",
    "- **Documentation:** Comprehensive technical guides and API references\n",
    "- **Community:** Professional network of healthcare data practitioners\n",
    "- **Expert Consultation:** Direct access to J&J data engineering teams\n",
    "\n",
    "**Begin your professional data transformation journey.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
